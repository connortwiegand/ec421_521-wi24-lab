---
title: "Lab 6: Simulations"
subtitle: "Application -- The Central Limit Theorem"
author: Connor Wiegand
format:
  html:
    theme: 
      - flatly
      - ../lab-style.scss
    toc: true
    toc-depth: 3
    execute: 
      # eval: false
      warning: false
      error: false
    embed-resources: true
    # html-math-method: mathjax
knitr: 
   opts_chunk: 
      class-output: code-out
      class-error: err-out
      class-message: msg-out
# editor:
#    render-on-save: true
---

```{r}
pacman::p_load(tidyverse, magrittr, cowplot)
```

# The Central Limit Theorem

## Theoretical Background

The [Central Limit Theorem (CLT)]{.hi} is a fundamental principle in statistics that describes the behavior of the mean of a large number of independent, identically distributed (i.i.d.) random variables. The theorem states that as the sample size becomes larger, the distribution of the sample mean approaches a normal distribution, regardless of the shape of the population distribution. The exact statement of the theorem is included below. 

:::{.callout-note collapse="true"}
### The Central Limit Theorem

*Let* $x_1, x_2, \dots, x_n$ *be a random sample from a population with population mean* $\mathop{\mathbb{E}}\left[ X \right] = \mu$ *and population variance* $\text{Var}\left( X \right) = \sigma^2 < \infty$. *Let* $\bar{X}$ *be the sample mean. Then, as* $n\rightarrow \infty$*, the function* 
$$\frac{\sqrt{n}\left(\bar{X}-\mu\right)}{S_x}$$

 *converges to a* [<u>*Normal Distribution*</u>]{.hi} *with mean 0 and variance 1.* 
:::


Suppose we have a population with mean $\mu$ and finite variance $\sigma^2$. If we take successive samples of size $n$ from this population, then the sample means ($\bar{X}$'s) will approximately follow a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$. Mathematically, this is expressed as:
$$
\bar{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
$$

This theorem is significant because it enables us to make inferences about population parameters using sample statistics, even when the population is not normally distributed.

## Simulating the Central Limit Theorem in R

Let's reiterate the concept of the [CLT]{.hi} in R! To demonstrate the Central Limit Theorem through simulation, we will follow these steps:

1. [Choose a Non-Normal Distribution]{.hi}: We'll start with a population distribution that is clearly not normal (e.g., a uniform or exponential distribution) to illustrate the power of the CLT.

2. [Draw Samples and Compute Means]{.hi}: We'll draw multiple samples of increasing size from this population and compute the mean of each sample.

3. [Visualize the Distribution of Sample Means]{.hi}: By plotting the distribution of these sample means, we'll observe how it becomes more bell-shaped and normal as the sample size increases, illustrating the CLT.

---

First, let's create a non-normal population distribution in R

```{r}
#| label: pop_distribution
#| echo: true

# Generate a random seed
set.seed(42)

# Set population size
population_size = 10000

# Generating a population from a uniform distribution
pop_tbl = tibble(
  value = 
    runif(population_size, min = 0, max = 10) + 
    rnorm(population_size, 0, 5)
)

glimpse(pop_tbl)
```

So we've created a (**non-normal**) population. Now, let's create a sampling distribution. We can do this using the `sample_n` function from `dplyr`, which is outlined below. 

::::::{.callout-note collapse="true"}
#### Function notes: `sample_n`

::: {.panel-tabset}

##### [Description]{.hi}

The `sample_n` function in R is used to randomly select a specified number of rows from a data frame or a `tibble`. This function is particularly useful for creating random subsets of a dataset, which can be essential for tasks like creating training and testing sets in machine learning, performing bootstrapping, or conducting random sampling for statistical analysis.

Here's a brief description of its usage and syntax:

- [Function]{.hi}: `sample_n()`
- [Package]{.hi}: `dplyr` (Tidyverse)

##### [Syntax]{.hi}
```R
sample_n(tbl, size, replace = FALSE, weight = NULL, .env = NULL)
```

- `tbl`: The data frame or tibble to sample from.
- `size`: The number of rows to sample. If this number is greater than the number of rows in the `tbl` and `replace` is `FALSE`, it throws an error. If `replace` is `TRUE`, it allows for sampling with replacement.
- `replace`: Logical argument indicating whether the sampling should be with replacement. Default is `FALSE`.
- `weight`: An optional vector of probabilities for selecting each row. It must be of the same length as the number of rows in the `tbl`.
- `.env`: An environment in which to evaluate the weights.

##### [Ex.]{.ex}
```{r}
library(dplyr)

data = tibble(
  value = rnorm(n = 10, mean = 0, sd = 1)
)

# To randomly select 5 rows from the data frame
sampled_data = sample_n(data, 5)

sampled_data
```

:::

::::::


We start by creating a sampling tibble, which we will call `sample_tbl`:

```{r}
#| echo: true
#| label: sampling_object

# Set sample size
sample_size = 50

# Randomly pick sample from population
sample_tbl = pop_tbl %>% 
  sample_n(., sample_size) 

sample_tbl
```

To take the mean, we can simply pass this `tbl` to `summarize`:

```{r}
sample_tbl %>% summarize(mu_hat = mean(value))
```

We need a lot of means in order to plot something that looks like a normal distribution. That means we need a lot of samples. Let's implement a `for` loop to help us understand how the [CLT]{.hi} works. We want our loop to:

- Repeatedly take samples from the population distribution
- Calculate the mean of the sample
- Store the means somewhere so we can plot them

Speaking of plotting, there are two properties of the [CLT]{.hi} that I want to visualize:

> 1. If we take sample of this distribution (or _any_ distribution), and find the mean, repeated sample means of this population will approach a normal distribution
>
>   2. As the sample size increases, the speed to which we approach the normal distribution increases

```{r}
#| label: sim

# Create an empty tibble to store our sample means
sim_tbl = tibble(mu_hat = numeric(0))

# Simulate the process 20 times
for (i in 1:20) {
  # Sampling and computing the mean
  sample_mean = pop_tbl %>% 
                 sample_n(size = sample_size) %>% 
                 summarize(mu_hat = mean(value)) %>% 
                 pull(mu_hat)
  
  # Adding the result to the tibble
  sim_tbl = bind_rows(sim_tbl, tibble(mu_hat = sample_mean))
}
```

We have our sample means, now let's plot the result:

```{r}
ggplot(sim_tbl, aes(mu_hat)) +
   geom_histogram(
      binwidth = 0.25, 
      color = "white", 
      fill = "#18bc9c") +
   theme_minimal()
```

The output looks fairly reasonable (normal...ish?). But, to better visualize the two properties highlighted above, let's beef up the scale of our simulation.

To capture (2.), I'm going to vary the sample size ($n$) and increase the number of times we run the sim. In the plot below, rows correspond to a fixed sample size (**n = **), while columns correspond a fixed number of samples (**iterations**). 

```{r}
#| label: real_sim_1
#| echo: false
#| cache: true
#| message: false
#| warning: false

library(cowplot)


sim_fun <- function(pop_tbl, sample_size, iter) {
   # Simulation
   sim_tbl <- 
      parallel::mclapply(
         mc.cores = 14, 
         X = 1:iter, 
         FUN = function(x, size = sample_size) {
            pop_tbl %>%
               sample_n(size = sample_size) %>%
               summarize(mu_hat = mean(value))
         }) %>%
      do.call(rbind, .) %>%
      as_tibble()

   p <- ggplot(data = sim_tbl, aes(mu_hat)) +
      geom_histogram(
         binwidth = 0.05, 
         color = "white", 
         lwd = 0.2, 
         fill = "#18bc9c"
         ) +
      theme_minimal() +
      labs(
         caption = 
            paste0(
               "(",
               iter, 
               " iterations, sample size = ", 
               sample_size,
               ")"
            ),
         x = "Sample mean",
         y = "Count"
         ) +
      coord_cartesian(xlim = c(2, 8)) +
      theme(plot.caption = element_text(hjust = 0.45))
}

# Defining parameter ranges
sample_sizes <- c(5, 10, 25, 50, 100, 200, 1000)
iter <- c(100, 1000, 10000)

# Applying the function and storing results in a nested list
grid <- lapply(X = iter, function(i) {
   lapply(X = sample_sizes, function(s) {
      sim_fun(sample_size = s, iter = i, pop_tbl = pop_tbl)
   })
})
```

```{r}
#| label: real_sim_2
#| echo: false
#| cache: true
#| message: false
#| warning: false

# Function to arrange plots for a specific iteration
arrange_plots_for_iter <- function(plots_list, iter_index) {
   hor_lab = ""
   if(iter_index == 1) { 
      hor_lab = unlist(lapply(
         "n = ", 
         paste0, 
         as.character(sample_sizes) 
      ))
   }
   
   plot_grid(plotlist = plots_list[[iter_index]], 
             ncol = 1,
             labels = hor_lab,
             label_y = 0.875,
             label_x = 0.05)
}

# Arrange the plots in a grid with each column having the same iteration
grid_plot <- plot_grid(
   arrange_plots_for_iter(grid, 1),
   arrange_plots_for_iter(grid, 2),
   arrange_plots_for_iter(grid, 3),
   ncol = 3,
   labels = c("100 Iterations", "1000 Iterations", "10000 Iterations"),
   label_size = 18,
   label_x = 0.32,
   hjust = 0,
   vjust = 1
)

```

```{r} 
#| echo: false
#| fig.height: 16
#| fig.width: 12


# Display or save the final grid plot
grid_plot
```

Again, here are the two properties we were hoping to analyze:

> 1. If we repeatdely take samples of a distribution, the sample means of will approach a normal distribution

This is captured column-wise[^i.e., look at a fixed row in the table of graphs]: as we increase the number of iterations, what do you notice?

>   2. As the sample size increases, the speed to which we approach the normal distribution increases

Now compare across rows: what do you notice about increasing $n$?

## Conclusion

The [CLT]{.hi} is a fundamental concept in econometrics, providing critical insight into the behavior of sample means and the normality of distributions. While its theoretical importance is undisputed, the application of simulations offers a complementary and practical approach to understanding these concepts. Simulations allow practitioners to visualize the convergence of sample means to normality as outlined by the CLT, especially with large sample sizes. This empirical approach not only reinforces the theoretical understanding but also provides intuitive insights into the stochastic nature of econometric models. By manipulating parameters and observing outcomes, econometricians can explore and validate theoretical assumptions, gaining a more nuanced appreciation of the underlying statistical phenomena.

Moreover, the combination of theory and simulation is invaluable in assessing the robustness of econometric models. Through simulated scenarios, the sensitivity and behavior of models under various conditions can be examined, which is particularly useful in policy analysis and forecasting. This dual approach not only deepens the understanding of econometric principles but also enhances the practitioner's ability to apply these principles in real-world situations. Ultimately, integrating theoretical knowledge with practical simulation exercises equips economists with a more comprehensive toolkit, bridging the gap between abstract concepts and empirical data analysis, and leading to more informed and reliable decision-making in the field.

## Resources

[3blue1brown](https://www.youtube.com/@3blue1brown) is a highly popular youtube channel which explains and visualizes concepts from various quantitative fields. If you would like a better understanding of the normal distribution, the [CLT]{.hi}, and how they related to convolutions and $\pi$, check out this series of videos:

- [What is the the Central Limit Theorem?](https://www.youtube.com/watch?v=zeJD6dqJ5lo)
- [A pretty reason why Gaussian + Gaussian = Gaussian](https://www.youtube.com/watch?v=d_qvLDhkg00)
- [Why $\pi$ is in the normal distribution](https://www.youtube.com/watch?v=cy8r7WSuT1I)
- [What is a convolution](https://www.youtube.com/watch?v=KuXjwB4LzSA)