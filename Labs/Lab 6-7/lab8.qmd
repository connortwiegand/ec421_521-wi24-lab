---
title: "Lab 8: Time Series"
author: Connor Wiegand
format:
  html:
    theme: 
      - flatly
      - ../lab-style.scss
    toc: true
    toc-depth: 3
    execute: 
      error: false
    embed-resources: true
    # html-math-method: mathjax
knitr: 
   opts_chunk: 
      class-output: code-out
      class-error: err-out
      class-message: msg-out
# editor:
#    render-on-save: true
---


# Background

## Introduction

Time series data is a sequence of observations recorded at regular time intervals. This type of data is prevalent as it allows us to study economic variables over time, such as GDP, inflation rates, stock prices, and many others.

Understanding time series data is crucial for predicting future trends, understanding past behaviors, and making informed decisions in the economic sphere.


## Time Series Basics

### Overview
Time series data is a sequence of data points collected or recorded at regular time intervals. This type of data is significant in economics as it helps in analyzing patterns over time. Common examples include:

- [Unemployment Rate Over Time]{.hi}: Helps understand the labor market dynamics.
- [Stock Prices Over Time]{.hi}: Essential for financial analysis and investment strategies.

### Regression in Time Series
Regression analysis in time series is used to understand relationships between different time-dependent variables. It involves using Ordinary Least Squares (OLS) to estimate the relationships. For example, you can regress GDP (as the dependent variable) on Bitcoin prices (as the independent variable) to study their relationship over time.

#### Types of regression models in time series

1. **Static Model**
   - [Definition:]{.hit} In a static model, the independent variables at a given time are used to predict the dependent variable at the same time.
   - [Example:]{.hit} Analyzing how today's GDP impacts today's Bitcoin price. This model assumes immediate effects of the independent variables on the dependent variable.
   - [Economic Significance:]{.hit} Useful for assessing immediate impacts and short-term analysis.

2. **Dynamic Model**
   - [Definition:]{.hit} Dynamic models take into account not just the current values of variables but also their past values. They are crucial for understanding lag effects in economics.
   - [Components:]{.hit} May include variables like:
     - X (independent variable) today and its past values (e.g., yesterday, last month).
     - Y (dependent variable) today and its past values.
   - [Example:]{.hit} Analyzing how today’s and yesterday’s GDP, along with yesterday's Bitcoin prices, affect today's Bitcoin price.
   - [Economic Significance:]{.hit} Helps in understanding the delayed effects of economic policies or market changes, vital for long-term economic planning and forecasting.

#### Key points 

- **Stationarity**: Ensuring that the time series is stationary (its properties do not change over time) is crucial for reliable statistical inference in regression analysis.
- **Autocorrelation**: Time series data often exhibit autocorrelation, where current values are correlated with past values. This needs to be accounted for in the analysis.


### Autocorrelation in Time Series

**Autocorrelation** refers to the correlation of a time series with its own past values. In econometrics, this concept is crucial because:

- **Error Terms**: If the error terms in a regression model are autocorrelated, the assumption of independence of error terms is violated. This can occur in time series data where the error at one point in time is correlated with the error at a previous time.
- **Consequences**: Autocorrelation can lead to biased and inefficient estimates, affecting the reliability of regression results.

### Detecting Autocorrelation
- **Visual Inspection**: Plotting the residuals of a time series model can help in visually identifying patterns of autocorrelation.
- **Statistical Tests**: Tests like the Durbin-Watson test are used to formally detect the presence of autocorrelation.

## Example: Generating an AR(1) Process in R

### Background

[Autoregressive (AR)]{.hi} models are fundamental to time series analysis. They are estimated via regressing a variable on one or more of its lagged values. We can simulate such processes in R, where each value in the series is a function of its past values plus a random error term. An AR(1) model, for instance, can be represented as:

$$
Y_t = c + \phi Y_{t-1} + \varepsilon_t
$$

where $Y_t$ is the value at time $t$, $c$ is a constant, $\phi$ is the coefficient of the previous time period's value, and $\epsilon_t$ is the error term.

:::{.callout-note collapse="true"}
#### General AR(p) model

An autoregressive model of order $p$ --- or [AR(p)]{.hi} model --- takes the form: 
$$
Y_t = c + \sum_{i=1}^{p} (\phi_i Y_{t-i}) + \varepsilon_t 
$$
:::

### R Code for Simulation and Plotting
```{r}
#Load our "standard" packages
pacman::p_load(tidyverse, magrittr)

# Set seed for reproducibility 
set.seed(123) 
# Series length (number of observations)
n = 100
# AR(1) coefficent
phi = 0.8
# Some constant
c = 5
# Random error
e = rnorm(n)

Y = rep(0, n)  # Placeholder for the time series

# Generating AR(1) series
Y[1] = c + e[1]  # First value
for (i in 2:n) {
  Y[i] = c + phi * Y[i-1] + e[i]
}

# Creating a dataframe for plotting
tbl = tibble(year = 1:n, value = Y)

head(tbl)
```

Now let's plot the entire series

```{r}
# Plotting the time series
ggplot(tbl, aes(x = year, y = value)) +
  geom_line() +
  hrbrthemes::theme_ipsum() +
  labs(title = "AR(1) Time Series",
       x = "Time",
       y = "Value")
```

------

# Autocorrelation in the Real World

## Testing for Autocorrelation

### Loading Time Series Data
First, we'll load the time series data containing GDP and government expenditure.

```{r}
#| message: false

# library(tidyverse)

fed_tbl = read_csv(url("https://raw.githubusercontent.com/qmatsuzawa/Datasets/main/Datasets/timeseries.csv")) %>% 
  as_tibble()
```

The two columns are:

1. GDP (`gdp`)
2. Government Expenditure (`gov`)

```{r}
glimpse(fed_tbl)
```

### Visual Inspection for Autocorrelation

#### Steps:
1. Perform regression and obtain residuals.
2. Plot residuals against their lagged values.

```{r}
#| warning: false
# Step 1: Regression and finding residuals
fit = lm(gdp ~ gov, data = fed_tbl)
fed_tbl = fed_tbl %>% 
  mutate(residuals = resid(fit))

# Step 2: Plotting residuals against lagged residuals
fed_tbl = fed_tbl %>% 
  mutate(lagged_residuals = lag(residuals))

ggplot(fed_tbl, aes(x = residuals, y = lagged_residuals)) +
  geom_point() +
  hrbrthemes::theme_ipsum() +
  labs(title = "Plot of Residuals vs. Lagged Residuals",
       x = "Residuals",
       y = "Lagged Residuals")
```

A positive correlation between residuals and lagged residuals suggests the presence of autocorrelation.

### Detecting Autocorrelation (Hyp. Testing)

:::{.callout-note collapse="true"}
#### The LM Test Statistic

When testing for serial correlation in the residuals, we must be careful about what our hypotheses are. It is straightforward to say that the null is "no autocorrelation", but we will need to be more precise in order to compute LM and conduct our test. In order to formulate our null hypothesis, we must have some guess regarding the *order* of the autoregressive process that the error terms follow. 

In general, if we think the errors follow an AR(p) process:

$$
u_{t} = \rho_1 u_{t-1} + \cdots + \rho_p u_{t-p} + \varepsilon_t 
$$

Then our null hypothesis is $H_{0}:\rho_1 = \cdots =\rho_p = 0$, and the LM test statistic is
$$
LM = (n-p)R^2
$$

using the appropriate $R^2$ (see below).
:::

#### Steps:
1. Regress the standard model.
2. Store residuals.
3. Regress residuals on their lagged values (using a number of lagged values consistent with the null hypothesis).
4. Store the $R^2$ from this regression.
5. Using (4), compute the LM test statistic and test according to $\chi^{2}_p$

Let's check for evidence of an $AR(2)$ error sequence in our data:

```{r}
# Step 1: Regression
fit = lm(gdp ~ gov, data = fed_tbl)

# Step 2: Storing residuals
fed_tbl = fed_tbl %>% 
  mutate(residuals = resid(fit))

# Step 3: Regression of residuals on lagged residuals
lm_test = lm(residuals ~ -1 + lag(residuals) + lag(residuals, 2), data = fed_tbl)

# Step 4: LM test statistic
n = nrow(fed_tbl)
p = 2
test_stat = (n - p) * summary(lm_test)$r.squared
pchisq(test_stat, df = p, lower.tail = FALSE)
```

In this hypothesis test, a very small p-value indicates the rejection of the null hypothesis of no autocorrelation, suggesting that an AR(2) process is present.


## Dealing w/ autocorrelation

### Check for misspecification
One way is to inspect whether the model is incorrectly specified. For example, maybe a static framework is NOT the right way to model our data. Maybe we want to lag Y and include that in our model. For instance, it is reasonable to think that the GDP of today is affected by GDP of yesterday...

Let's try to regress Y on X and lag(Y)   
```{r}
lm(gdp ~ gov + lag(gdp), fed_tbl) %>% broom::tidy()
```

The estimated coefficient is more reasonable. What does  visual inspection reveal?

```{r}
fit = lm(gdp ~ gov + lag(gdp), fed_tbl) 
fed_tbl$e = c(NA,resid(fit))
fed_tbl$lag_e = lag(fed_tbl$e)
ggplot(fed_tbl, aes(x=e, y=lag_e)) + geom_point()
```

The graph above does not provide much visual evidence of correlated residuals across time. Let's see how robust this claim is to hypothesis testing:

```{r}
### Step 1. Regress Y on X
fit = lm(gdp ~ gov + lag(gdp), fed_tbl) 

## Step 2. Find residual
fed_tbl$e = c(NA,resid(fit))

## Step 3. Regress residual on lagged resid
reg = lm(e ~ -1 + lag(e) + lag(e,2), fed_tbl)

## Step 4. Hypothesis testing using Chi-squared
n = nrow(fed_tbl)
p = 2
teststat = (n - p) * summary(reg)$r.squared
pchisq(teststat, p, lower.tail=F)
```

Because the $p-$value $> 0.1$, we fail to reject our null hypothesis of non-autocorrelation.

### Summary

- We had autocorrelation in our data
- By specifying a different model -- namely, one with lags -- we may have fixed the problem of autocorrelation 

------

# Feasible Generalized Least Squares (FGLS)

Feasible Generalized Least Squares (FGLS) is an econometric technique used to address violations of the standard Ordinary Least Squares (OLS) assumptions, particularly homoskedasticity and no autocorrelation. It's particularly useful in time series data that exhibits autocorrelation. FGLS transforms the model to counteract the effects of autocorrelation, resulting in more efficient and reliable estimates. The process involves estimating the autocorrelation within the residuals of a preliminary OLS regression and then using this estimation to adjust the original model.

## Steps to Conduct FGLS

### 1 Determine the Transformation
   - Suppose you suspect an AR(1) process in your time series data:
   
   $$\varepsilon_t = \rho \varepsilon_{t-1} + \nu_t$$
   
   Our model can then be transformed as:

$$
\begin{aligned}
 &Y_t = \beta_0 + \beta_1 X_t + \varepsilon_t \\
 \implies\quad &Y_t  - \rho Y_{t-1} = \beta_0+ \beta_1 X_t + \varepsilon_t - \rho Y_{t-1} \\
% & = \beta_0 + \beta_1 X_t + \varepsilon_t - \rho (\beta_0 + \beta_1 X_{t-1} + \varepsilon_{t-1}) \\
% & = \beta_0 (1-\rho) + \beta_1 (X_t - \rho X_{t-1}) + (\varepsilon_t - \rho \varepsilon_{t-1})  \\
\implies\quad &Y_t  - \rho Y_{t-1} = \beta_0 (1-\rho) + \beta_1 (X_t - \rho X_{t-1}) + \nu_t
\end{aligned}
$$

### 2 Estimate the Autocorrelation Parameter ($\rho$)
   - Perform a regression to find residuals and regress these on their lagged values (without an intercept) to estimate $\rho$.

```{r}
# Estimating rho
fit = lm(gdp ~ gov, data = fed_tbl)
fed_tbl$e = resid(fit)
lm(e ~ lag(e) - 1, data = fed_tbl)
fed_tbl$rho = 0.6384  # Example value from regression
```

### 3 Transform and Estimate the Model
   - Apply the estimated autocorrelation parameter to transform the variables in your model and re-estimate it.

```{r}
# Transforming and estimating the FGLS model
fit_fgls = lm(
   I(gdp - lag(gdp, 1) * fed_tbl$rho) ~ 
      -1 + 
      I(1 + fed_tbl$rho) + 
      I(gov - lag(gov, 1) * fed_tbl$rho),
   data = fed_tbl)
broom::tidy(fit_fgls)
```

### 4 Inspect the Residuals
   - Inspect the residuals of the FGLS model to check for reduced autocorrelation.

```{r}
# Checking residuals post-FGLS
fed_tbl = fed_tbl %>% 
  mutate(residuals = c(NA, resid(fit_fgls)),
         lagged_residuals = lag(residuals))

ggplot(fed_tbl, aes(x = residuals, y = lagged_residuals)) +
  geom_point() +
  labs(title = "Residuals After FGLS Transformation",
       x = "Transformed Residuals",
       y = "Lagged Transformed Residuals")
```

The reduced pattern in the residuals plot after applying FGLS suggests a decrease in autocorrelation, indicating the effectiveness of the FGLS method.


## Standard Errors

### Newey West

You can use ``se = "NW"`` option inside ``feols`` function to adjust your SEs.

- Note: You need variables panel, which for time series data, you should just set it equal to 1 for everyone.

```{r}
# pacman::p_load(fixest)
fed_tbl$panel = 1
fixest::feols(gdp ~ gov,
              fed_tbl,
              se = "NW",
              panel.id = ~panel+year
              )
```